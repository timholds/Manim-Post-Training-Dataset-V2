2025-06-19 13:34:43,212 - INFO - Loading model: unsloth/Qwen2.5-Coder-1.5B-Instruct-bnb-4bit
==((====))==  Unsloth 2025.6.2: Fast Qwen2 patching. Transformers: 4.52.4.
   \\   /|    NVIDIA RTX A4500 Laptop GPU. Num GPUs = 1. Max memory: 15.609 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth 2025.6.2 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.
2025-06-19 13:34:50,968 - INFO - Model and tokenizer initialized successfully
2025-06-19 13:34:50,969 - INFO - Loading dataset from data_formatted/train.json
2025-06-19 13:34:51,004 - INFO - Loaded 634 samples
2025-06-19 13:34:51,006 - INFO - Loading dataset from data_formatted/test.json
2025-06-19 13:34:51,018 - INFO - Loaded 100 samples
2025-06-19 13:34:51,034 - INFO - Model has 1,562,179,072 parameters
2025-06-19 13:34:51,059 - INFO - Trainable parameters: 18,464,768 (1.18%)
Unsloth: Tokenizing ["text"]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 634/634 [00:00<00:00, 3518.69 examples/s]
Unsloth: Tokenizing ["text"]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 6962.08 examples/s]
2025-06-19 13:34:51,706 - INFO - Starting training from scratch...
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 634 | Num Epochs = 3 | Total steps = 120
O^O/ \_/ \    Batch size per device = 4 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16
 "-____-"     Trainable parameters = 18,464,768/1,500,000,000 (1.23% trained)
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                               | 50/120 [02:19<02:55,  2.51s/it]Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.
{'loss': 2.0897, 'grad_norm': 0.5646125674247742, 'learning_rate': 0.0, 'epoch': 0.03}
{'loss': 1.9792, 'grad_norm': 0.6650816202163696, 'learning_rate': 0.00015000000000000001, 'epoch': 0.25}
{'loss': 1.299, 'grad_norm': 0.7290323376655579, 'learning_rate': 0.00019793406217655517, 'epoch': 0.5}
{'loss': 0.9559, 'grad_norm': 1.1606923341751099, 'learning_rate': 0.00018802013911801112, 'epoch': 0.75}
{'loss': 0.8817, 'grad_norm': 1.9701570272445679, 'learning_rate': 0.00017071067811865476, 'epoch': 1.0}
{'loss': 0.8227, 'grad_norm': 0.30373457074165344, 'learning_rate': 0.00014746003697476404, 'epoch': 1.25}
Using gradient accumulation will be very slightly less accurate.
Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [05:22<00:00,  2.69s/it]
2025-06-19 13:40:15,227 - INFO - Saving LoRA adapters...                                                               
{'eval_loss': 0.8702134490013123, 'eval_runtime': 6.6559, 'eval_samples_per_second': 15.024, 'eval_steps_per_second': 3.756, 'epoch': 1.25}
{'loss': 0.7702, 'grad_norm': 0.3322593569755554, 'learning_rate': 0.00012022175723320381, 'epoch': 1.5}
{'loss': 0.7291, 'grad_norm': 0.3453215956687927, 'learning_rate': 9.128442572523417e-05, 'epoch': 1.75}
{'loss': 0.7369, 'grad_norm': 0.43752917647361755, 'learning_rate': 6.307938526873157e-05, 'epoch': 2.0}
{'loss': 0.6783, 'grad_norm': 0.40261736512184143, 'learning_rate': 3.7976450873174005e-05, 'epoch': 2.25}
{'loss': 0.6424, 'grad_norm': 0.43577054142951965, 'learning_rate': 1.808479557110081e-05, 'epoch': 2.5}
{'eval_loss': 0.8469293117523193, 'eval_runtime': 5.6816, 'eval_samples_per_second': 17.601, 'eval_steps_per_second': 4.4, 'epoch': 2.5}
{'loss': 0.6152, 'grad_norm': 0.3988008201122284, 'learning_rate': 5.075735642696611e-06, 'epoch': 2.75}
{'loss': 0.6285, 'grad_norm': 0.5140580534934998, 'learning_rate': 4.230499177994007e-08, 'epoch': 3.0}
{'train_runtime': 322.7103, 'train_samples_per_second': 5.894, 'train_steps_per_second': 0.372, 'train_loss': 0.8958383798599243, 'epoch': 3.0}
2025-06-19 13:40:15,802 - INFO - Saving merged 16-bit model to models/merged_model...
Found HuggingFace hub cache directory: /home/timholds/.cache/huggingface/hub
Checking cache directory for required files...
Cache check failed: model.safetensors not found in local cache.
Not all required files found in cache. Will proceed with downloading.
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.09G/3.09G [01:54<00:00, 27.0MB/s]
Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.25s/it]
2025-06-19 13:42:17,686 - INFO - Training completed successfully!
2025-06-19 13:42:17,686 - INFO - Final training loss: N/A

2025-07-10 16:58:10,097 - INFO - Loading model: Qwen/Qwen2.5-Coder-1.5B-Instruct
2025-07-10 16:58:10,098 - INFO - Found Unsloth version: unsloth/Qwen2.5-Coder-1.5B-Instruct-bnb-4bit
2025-07-10 16:58:10,098 - INFO - Using Unsloth optimized model: unsloth/Qwen2.5-Coder-1.5B-Instruct-bnb-4bit
==((====))==  Unsloth 2025.6.2: Fast Qwen2 patching. Transformers: 4.52.4.
   \\   /|    NVIDIA RTX A4500 Laptop GPU. Num GPUs = 1. Max memory: 15.525 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth 2025.6.2 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.
2025-07-10 16:58:17,260 - INFO - Model and tokenizer initialized successfully
2025-07-10 16:58:17,260 - INFO - Loading dataset from data_formatted/train.json
2025-07-10 16:58:17,301 - INFO - Loaded 634 samples with qwen formatting
2025-07-10 16:58:17,302 - INFO - Loading dataset from data_formatted/test.json
2025-07-10 16:58:17,307 - INFO - Loaded 100 samples with qwen formatting
2025-07-10 16:58:17,310 - INFO - Model has 1,562,179,072 parameters
2025-07-10 16:58:17,315 - INFO - Trainable parameters: 18,464,768 (1.18%)
Unsloth: Tokenizing ["text"]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 634/634 [00:00<00:00, 4746.65 examples/s]
Unsloth: Tokenizing ["text"]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 7828.70 examples/s]
2025-07-10 16:58:17,810 - INFO - Resuming from checkpoint: models/Qwen_Qwen2.5-Coder-1.5B-Instruct_lora/checkpoint-50
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 634 | Num Epochs = 3 | Total steps = 120
O^O/ \_/ \    Batch size per device = 4 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16
 "-____-"     Trainable parameters = 18,464,768/1,500,000,000 (1.23% trained)
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 100/120 [02:20<00:47,  2.37s/it]Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.
{'loss': 0.0879, 'grad_norm': 0.3393733501434326, 'learning_rate': 0.00012022175723320381, 'epoch': 1.5}
{'loss': 0.054, 'grad_norm': 0.32147127389907837, 'learning_rate': 9.128442572523417e-05, 'epoch': 1.75}
{'loss': 0.0394, 'grad_norm': 0.25214681029319763, 'learning_rate': 6.307938526873157e-05, 'epoch': 2.03}
{'loss': 0.0296, 'grad_norm': 0.25280874967575073, 'learning_rate': 3.7976450873174005e-05, 'epoch': 2.28}
{'loss': 0.0233, 'grad_norm': 0.17105166614055634, 'learning_rate': 1.808479557110081e-05, 'epoch': 2.53}
Using gradient accumulation will be very slightly less accurate.
Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 119/120 [03:13<00:01,  1.63s/it]
2025-07-10 17:01:32,203 - INFO - Saving LoRA adapters...                                                                
{'eval_loss': 2.1102194786071777, 'eval_runtime': 7.0817, 'eval_samples_per_second': 14.121, 'eval_steps_per_second': 3.53, 'epoch': 2.53}
{'loss': 0.0207, 'grad_norm': 0.17101123929023743, 'learning_rate': 5.075735642696611e-06, 'epoch': 2.78}
{'train_runtime': 193.5937, 'train_samples_per_second': 9.825, 'train_steps_per_second': 0.62, 'train_loss': 0.023262645642296607, 'epoch': 3.0}
2025-07-10 17:01:32,863 - INFO - Training completed successfully!
2025-07-10 17:01:33,084 - INFO - Cleaning up GPU resources...
2025-07-10 17:01:34,364 - INFO - âœ“ GPU cleanup completed
